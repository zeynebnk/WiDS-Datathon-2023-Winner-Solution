{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tabgan;\n# !pip install autogluon;\n# !pip install optuna;\n# !pip install pytorch-tabnet;","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:05.101548Z","iopub.execute_input":"2023-03-11T04:07:05.103549Z","iopub.status.idle":"2023-03-11T04:07:05.110890Z","shell.execute_reply.started":"2023-03-11T04:07:05.103466Z","shell.execute_reply":"2023-03-11T04:07:05.108449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport datetime as dt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from datetime import datetime\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RandomizedSearchCV, GridSearchCV, GroupShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom catboost import CatBoostRegressor, Pool\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport gc\nimport lightgbm as lgb\n%matplotlib inline\nfrom tqdm.notebook import tqdm\nimport pickle\nimport shap\nfrom bayes_opt import BayesianOptimization\n\n# import torch\n# from autogluon.tabular import TabularDataset, TabularPredictor\n# import optuna\n# from optuna import Trial, visualization\n# from pytorch_tabnet.tab_model import TabNetRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:05.430353Z","iopub.execute_input":"2023-03-11T04:07:05.431093Z","iopub.status.idle":"2023-03-11T04:07:05.450870Z","shell.execute_reply.started":"2023-03-11T04:07:05.431056Z","shell.execute_reply":"2023-03-11T04:07:05.449421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hello all!\n\nI'm Zeyneb, a junior at Saratoga High School and the High School Winner of this year's WiDS Datathon! I am also a WiDS Ambassador from the Bay Area. I have experience working with data science and machine learning in projects and research, yet this was a relatively new area of application for me--I usually work with NLP and text. I joined a little later on in the competition, about February. It was really and amazing oppertunity to learn and I was able to apply a lot of new skills and learn new methods working on such and interesting problem to solve real-world problems. I had a lot of fun, and I would love to thank everyone at WiDS for making these events so that people like me can really grow and experience *real* data science! I got to explore a lot of new things working on this, and I wanted to share my approach and findings in the problem! Please don't hesitate to contact me with any questions or comments! ","metadata":{}},{"cell_type":"markdown","source":"Data Exploration:\n- Right from the start, the data presented some very interesting characteristics. There is a looong time gap between train and test data, a significant differnece in the value distritbutions of the features, and outliers that seems to have comparably more/less temperature change over time. Of course, these can result in some interesting behaviors in the models, explored later below.\n- I explored and based some code on the following notebooks (Thank you!):\n    - https://www.kaggle.com/code/nicholasdominic/wids2023-data-buddies\n    - https://www.kaggle.com/code/kooaslansefat/wids-2023-bo-catboost-xai-pseudo-labeling\n    - https://www.kaggle.com/code/kooaslansefat/wids-2023-woman-life-freedom/notebook\n    \nInteresting/Unintuitive Findings (?):\n- In initial experiments, I label encoded features with ~50 distinct values in the train data as categorical features when building the model, then selected top (30%-15%) features based on shapley values. I also performed hyperparameter tuning with 3-5 fold cross validation (split based on location i.e., 'loc_group', or random split). The final output performed relatively poorly (RMSE ~1.5). Strangely , specifying categorical features explicitly into the CatBoost and LightGBM models decreased performance, as did location based splitting. Despite the risks of overfitting and data leakage, random splits and less feature selection seemed to do better. \n- Furthermore, high number of boosting iterations (20K+) with small learning rate also did better \n- I expected overfitting and shakeup in the private leaderboard but I decided to explore and experiment with these.\n\nExperiments that DIDN'T make the final approach:\n- Several forecasts for target variable are included in the data. I wanted to use these forecasts as proxy for target. (Strangely, it seems forecasts are constant for the first half of each month). I calculated average daily distance between forecasts and target value from training data (location-day_of_year), and used this as a feature for both train and test. I also added this value to test forecasts to estimate target. \n- I experimented with TabNet and AutoGluon as well for the models. I also worked with some augmentation methods like TabGAN and MixUp to generate synthetic data similar to the test set with pseudolabels. This helped however I have had inconsistent results over multiple runs, so I decided to leave them for replicability.\n- Climate Region \"Expert\" Models: I have also experimented with building a CatBoost model for each climate region with complete training data. I assigned higher sample weights for a that climate region and used that model to predict that specific climate region's test data. For example, for climate region = 'BSk', I assigned sample weights for training data where if the region is BSk then I assigned 1, 0.33 otherwise. I built a CatBoost model and then I used this model to predict only climate region BSk. This also helped improve RMSE.  \n\nFinal Approach:\n- A key method I used is \"iterative pseudolabeling\" the test data to incoperate into the training, explained further.\n- I primarilly worked with CatBoost and LightGBM models, combining train and pseudo labeled test data, and ensemble the resulting models with previous steps' ensembled model. I iterate over 2 times and finally ensemble with climate region experts. While ensembling, the idea that I used is favoring the most recent model more, and use the ensembled (new and previous step's model) predictions only if the absolute difference between these predictions is below a certain threshold that is set based on experiment. If they differ a lot then use the new model. \nFinal Approach Steps:\n1. Build LightGBM and CatBoost models, LGB0 and CB0.\n2. Build CatBoost Model, CB1, on train data with another set of hyperparameters (2K iterations)\n3. Pseudolabel test data from CB1 predictions and build CatBoost model, CB_PL0, with the same hyperparameters from step 2 on train+test data. \n4. Ensemble LGB0, CB0 and CB_PL0 models and use these predictions to pseudolabel test data and build a CatBoost model CB_PL1 with the same CatBoost hyperparameters as Steps 2 and 3, but increase iterations to 25K. Also build LightGBM model, LGB_PL1. Ensemble these two models, also ensemble with the previously ensembled models. i.e., New predictions are from ensembling LGB0, CB0, CB_PL0, LGB_PL1 and CB_PL1. \n5. Use the latest predictions to pseudolabel tets data and build climate expert models. Ensemble climate experts predictions with the latest ensemble to get the final submission. \n- The models run in just over an hour.","metadata":{}},{"cell_type":"markdown","source":"## 1. Some EDA","metadata":{"execution":{"iopub.status.busy":"2023-03-11T03:55:12.999399Z","iopub.execute_input":"2023-03-11T03:55:13.000827Z","iopub.status.idle":"2023-03-11T03:55:13.006034Z","shell.execute_reply.started":"2023-03-11T03:55:13.000715Z","shell.execute_reply":"2023-03-11T03:55:13.004919Z"}}},{"cell_type":"code","source":"train_raw = pd.read_csv('/kaggle/input/widsdatathon2023/train_data.csv', parse_dates=[\"startdate\"])\ntest_raw = pd.read_csv('/kaggle/input/widsdatathon2023/test_data.csv', parse_dates=[\"startdate\"])\ntarget = 'contest-tmp2m-14d__tmp2m'\n\ntest_raw[target] = 0\ntrain_raw['is_train'] = 1\ntest_raw['is_train'] = 0\nall_data = pd.concat([train_raw, test_raw])\n\ndel train_raw, test_raw\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:06.638871Z","iopub.execute_input":"2023-03-11T04:07:06.639280Z","iopub.status.idle":"2023-03-11T04:07:37.667341Z","shell.execute_reply.started":"2023-03-11T04:07:06.639249Z","shell.execute_reply":"2023-03-11T04:07:37.666122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_candidates = ['elevation__elevation', 'mjo1d__phase', 'mei__meirank', 'mei__nip', 'loc_group']\n\n# Create Location id\nlocation_group = ['lat', 'lon']\nscale = 14\n\nall_data.loc[:,'lat']=round(all_data.lat,scale)\nall_data.loc[:,'lon']=round(all_data.lon,scale)\n\nall_data = all_data.sort_values(location_group) \nall_data['loc_group'] = all_data.groupby(location_group, dropna = False).ngroup()\nall_data.loc_group.nunique()\n\nall_data = all_data.sort_values('lat') \nall_data['lat_group'] = all_data.groupby('lat', dropna = False).ngroup()\nall_data.lat_group.nunique()\n\nall_data = all_data.sort_values('lon') \nall_data['lon_group'] = all_data.groupby('lon', dropna = False).ngroup()\nall_data.lat_group.nunique()\n\nall_data['startdate'] = pd.to_datetime(all_data['startdate'])\nall_data['year'] = all_data['startdate'].dt.year\nall_data['month'] = all_data['startdate'].dt.month\nall_data['day_of_year'] = all_data['startdate'].dt.dayofyear\n\nle = LabelEncoder()\nall_data['climateregions__climateregion'] = le.fit_transform(all_data['climateregions__climateregion'])\n\ndef encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n    return data\n\nall_data['month'] = all_data.startdate.dt.month\nall_data = encode(all_data, 'month', 12)\n\nall_data['day'] = all_data.startdate.dt.day\nall_data = encode(all_data, 'day', 31)\n\nall_data = encode(all_data, 'day_of_year', 365)\n\nfor c in cat_candidates:\n    all_data[c] = all_data[c].astype('int64')","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:37.669900Z","iopub.execute_input":"2023-03-11T04:07:37.670313Z","iopub.status.idle":"2023-03-11T04:07:39.748651Z","shell.execute_reply.started":"2023-03-11T04:07:37.670273Z","shell.execute_reply":"2023-03-11T04:07:39.747391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = all_data[all_data.is_train == 1]\ntrain = train.reset_index()\ntest = all_data[all_data.is_train == 0]\ntest = test.reset_index()\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:39.750348Z","iopub.execute_input":"2023-03-11T04:07:39.750666Z","iopub.status.idle":"2023-03-11T04:07:40.293157Z","shell.execute_reply.started":"2023-03-11T04:07:39.750635Z","shell.execute_reply":"2023-03-11T04:07:40.291637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features to be used as categorical","metadata":{}},{"cell_type":"code","source":"for i in train.columns.tolist():\n    if i in ['is_train']:\n        continue\n    if train[i].nunique() < 50:\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:40.295909Z","iopub.execute_input":"2023-03-11T04:07:40.296365Z","iopub.status.idle":"2023-03-11T04:07:41.180665Z","shell.execute_reply.started":"2023-03-11T04:07:40.296328Z","shell.execute_reply":"2023-03-11T04:07:41.179519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_na_cols = []\ntest_na_cols = []\nfor f in train.columns.tolist():\n    if train[f].isna().sum() > 0:\n        train_na_cols.append(f)\n    if test[f].isna().sum() > 0:\n        test_na_cols.append(f)\n        \ntrain_na_cols, test_na_cols","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:41.181696Z","iopub.execute_input":"2023-03-11T04:07:41.181988Z","iopub.status.idle":"2023-03-11T04:07:41.375495Z","shell.execute_reply.started":"2023-03-11T04:07:41.181955Z","shell.execute_reply":"2023-03-11T04:07:41.373828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features with na (Test doesn't have any) ","metadata":{}},{"cell_type":"code","source":"train.loc_group.nunique(), len([x for x in train.loc_group.unique() if x not in test.loc_group.unique()]), len([x for x in test.loc_group.unique() if x not in train.loc_group.unique()])","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:41.377821Z","iopub.execute_input":"2023-03-11T04:07:41.378504Z","iopub.status.idle":"2023-03-11T04:07:42.240314Z","shell.execute_reply.started":"2023-03-11T04:07:41.378460Z","shell.execute_reply":"2023-03-11T04:07:42.239333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 514 locations","metadata":{}},{"cell_type":"code","source":"all_data.groupby(['is_train', 'year', 'month'])['loc_group'].count().reset_index().sort_values(['is_train', 'year', 'month'])","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:42.241520Z","iopub.execute_input":"2023-03-11T04:07:42.241840Z","iopub.status.idle":"2023-03-11T04:07:42.272838Z","shell.execute_reply.started":"2023-03-11T04:07:42.241813Z","shell.execute_reply":"2023-03-11T04:07:42.271356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test has months 11 and 12","metadata":{"execution":{"iopub.status.busy":"2023-03-11T03:51:46.824314Z","iopub.execute_input":"2023-03-11T03:51:46.824681Z","iopub.status.idle":"2023-03-11T03:51:46.831778Z","shell.execute_reply.started":"2023-03-11T03:51:46.824650Z","shell.execute_reply":"2023-03-11T03:51:46.830398Z"}}},{"cell_type":"code","source":"train.groupby('climateregions__climateregion')['loc_group'].nunique().reset_index().sort_values('loc_group')","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:42.274704Z","iopub.execute_input":"2023-03-11T04:07:42.275225Z","iopub.status.idle":"2023-03-11T04:07:42.301914Z","shell.execute_reply.started":"2023-03-11T04:07:42.275179Z","shell.execute_reply":"2023-03-11T04:07:42.300786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Climate regions 0, 5, 12, 13 and 14 have fewer than 10 locations within them","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(7,5)})\ntmp = train[(train.climateregions__climateregion == 1) & (train.year == 2015) & (train.month.isin([11, 12]))]\nsns.lineplot(data = tmp, x = 'day_of_year', y= target, hue = 'loc_group')\nplt.title('Target over months 11, 12, year 2015, climate region BSk (1)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:42.302973Z","iopub.execute_input":"2023-03-11T04:07:42.303346Z","iopub.status.idle":"2023-03-11T04:07:43.712517Z","shell.execute_reply.started":"2023-03-11T04:07:42.303311Z","shell.execute_reply":"2023-03-11T04:07:43.711437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"region = 14 #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\ny = 2015\nsns.set(rc={'figure.figsize':(10,7)})\nforecasts = [ \n    'nmme-tmp2m-34w__cancm3',\n 'nmme-tmp2m-34w__cancm4',\n 'nmme-tmp2m-34w__ccsm3',\n 'nmme-tmp2m-34w__ccsm4',\n 'nmme-tmp2m-34w__cfsv2',\n 'nmme-tmp2m-34w__gfdl',\n 'nmme-tmp2m-34w__gfdlflora',\n 'nmme-tmp2m-34w__gfdlflorb',\n 'nmme-tmp2m-34w__nasa', \n 'nmme-tmp2m-34w__nmmemean'\n]\nlocs = train[(train.climateregions__climateregion == region) & (train.year == 2015) & (train.month.isin([11, 12]))].loc_group.unique().tolist()[:2]\nfor l in locs:\n    tmp3 = train[(train.loc_group == l) & (train.year == 2015) & (train.month.isin([11, 12]))].copy()\n    for f in forecasts:\n        sns.lineplot(data = tmp3, x = 'day_of_year', y= f, label = f)\n    sns.lineplot(data = tmp3, x = 'day_of_year', y= target, label = '2015 train target', linewidth = 3)\n   # plt.title( 'region: '+ str(tmp3.climateregions__climateregion.unique()[0])+  ' loc: '+ str(l))\n    plt.legend(loc='lower left', fontsize = 10)\n    plt.xlabel(target)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:43.715269Z","iopub.execute_input":"2023-03-11T04:07:43.715964Z","iopub.status.idle":"2023-03-11T04:07:44.242785Z","shell.execute_reply.started":"2023-03-11T04:07:43.715897Z","shell.execute_reply":"2023-03-11T04:07:44.241809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = train[(train.month == 12) & (train.year == 2015)][['lat_group', 'lon_group', target]].groupby(['lat_group', 'lon_group'])[target].mean().reset_index()\ntmp = tmp.pivot('lon_group','lat_group', target)\ntmp.sort_index(level=0, ascending=False, inplace=True)\nsns.set(rc={'figure.figsize':(10,7)})\nsns.heatmap(tmp)\nplt.title('Average target (~temperature) over month 12')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:44.244077Z","iopub.execute_input":"2023-03-11T04:07:44.248337Z","iopub.status.idle":"2023-03-11T04:07:44.776548Z","shell.execute_reply.started":"2023-03-11T04:07:44.248263Z","shell.execute_reply":"2023-03-11T04:07:44.775530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = train.copy()\ntmp = tmp[['lat', 'lon', 'climateregions__climateregion']].drop_duplicates()\nsns.set(rc={'figure.figsize':(10,7)})\nsns.scatterplot(data = tmp, x = 'lat', y = 'lon', hue = 'climateregions__climateregion', s = 200);","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:44.778151Z","iopub.execute_input":"2023-03-11T04:07:44.778829Z","iopub.status.idle":"2023-03-11T04:07:45.372421Z","shell.execute_reply.started":"2023-03-11T04:07:44.778790Z","shell.execute_reply":"2023-03-11T04:07:45.371323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train, test, all_data, train_na_cols, test_na_cols, tmp, tmp3, region, y, locs, forecasts\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:07:45.373651Z","iopub.execute_input":"2023-03-11T04:07:45.373955Z","iopub.status.idle":"2023-03-11T04:07:45.583903Z","shell.execute_reply.started":"2023-03-11T04:07:45.373926Z","shell.execute_reply":"2023-03-11T04:07:45.582286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Final Model","metadata":{}},{"cell_type":"code","source":"train_raw = pd.read_csv('/kaggle/input/widsdatathon2023/train_data.csv', parse_dates=[\"startdate\"])\ntest_raw = pd.read_csv('/kaggle/input/widsdatathon2023/test_data.csv', parse_dates=[\"startdate\"])\nsubmit = pd.read_csv('/kaggle/input/widsdatathon2023/sample_solution.csv')\ntarget = 'contest-tmp2m-14d__tmp2m'\nfname = '/kaggle/working/'\n# models = {}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(actual, predicted):\n    return mean_squared_error(actual, predicted, squared=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train XGBoost and LightGBM models\ndef location_feature_db(train, test):\n    # Reference: https://www.kaggle.com/code/flaviafelicioni/wids-2023-different-locations-train-test-solved\n    scale = 14\n    train.loc[:,'lat']=round(train.lat,scale)\n    train.loc[:,'lon']=round(train.lon,scale)\n    test.loc[:,'lat']=round(test.lat,scale)\n    test.loc[:,'lon']=round(test.lon,scale)\n    \n    train_and_test = pd.concat([train, test], axis=0)\n    train_and_test['loc_group'] = train_and_test.groupby(['lat', 'lon']).ngroup()\n    print(f'{train_and_test.loc_group.nunique()} unique locations')\n    \n    train = train_and_test.iloc[:len(train)]\n    test = train_and_test.iloc[len(train):].drop(target, axis=1)\n    \n    return train, test\n\ndef cat_encode_db(train, test):\n    # encoding the categorical feature in the train and test data set\n    # using OneHotEncoder\n    ohe = OneHotEncoder()\n    train_encoded = ohe.fit_transform(train[['climateregions__climateregion']])\n    test_encoded = ohe.transform(test[['climateregions__climateregion']])\n    \n    train = train.drop(['climateregions__climateregion'], axis=1)\n    test = test.drop(['climateregions__climateregion'], axis=1)\n    \n    train_encoded = pd.DataFrame(train_encoded.toarray(), columns=ohe.get_feature_names_out(['climateregions__climateregion']))\n    test_encoded = pd.DataFrame(test_encoded.toarray(), columns=ohe.get_feature_names_out(['climateregions__climateregion']))\n    \n    train = pd.concat([train, train_encoded], axis=1)\n    test = pd.concat([test, test_encoded], axis=1)\n    \n    return train, test\n    \ndef fill_na_rows_db(dataset):\n    # Find the columns with missing values\n    columns_with_missing_values = dataset.columns[dataset.isnull().any()].tolist()\n    \n    # Impute the missing values with the mean value of that column\n    for col in columns_with_missing_values:\n        dataset[col].fillna(dataset[col].mean(), inplace=True)\n        \n    return dataset\n\ndef create_new_feat_db(dataset):\n    dataset['year']=pd.DatetimeIndex(dataset['startdate']).year \n    dataset['month']=pd.DatetimeIndex(dataset['startdate']).month \n    dataset['day']=pd.DatetimeIndex(dataset['startdate']).day\n    return dataset\n\ndef identify_correlated_db(df, threshold):\n    corr_matrix = df.corr().abs()\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    reduced_corr_matrix = corr_matrix.mask(mask)\n    features_to_drop = [c for c in reduced_corr_matrix.columns if any(reduced_corr_matrix[c] > threshold)]\n    return features_to_drop\n\ndef feature_engineering_db(origin_train, origin_test):\n    train, test = origin_train, origin_test\n    train = fill_na_rows_db(train)\n    train = create_new_feat_db(train)\n    test = create_new_feat_db(test)\n    train, test = cat_encode_db(train, test)\n    irrelevant_cols = ['index', 'startdate', 'climateregions__climateregion', target]\n    features = [col for col in train.columns if col not in irrelevant_cols]\n    X = train[features]\n    X_test = test[features + ['index']]\n    y = train[target]\n    return X, y, X_test\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_db, y_db, X_test_db = feature_engineering_db(train_raw.copy(), test_raw.copy())\n\ncorrelation_threshold = 0.80\nfeatures_to_drop = identify_correlated_db(train_raw.copy(), correlation_threshold)\nremove_feature = ['index', target]\nfeatures_to_drop_v1 = [ele for ele in features_to_drop if ele not in remove_feature]\n\nprint(len(features_to_drop_v1),features_to_drop_v1)\n\nX_db.drop(features_to_drop_v1, axis=1, inplace = True)\nprint(\"Dropped features that are highly correlated\")\nX_train, X_val, y_train, y_val = train_test_split(X_db, y_db, test_size=0.33, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = X_db.copy(), y_db.copy()\nfeatures = X_train.columns.tolist()\n\nlgb_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_leaves': 31,\n    'max_depth': 8,\n    'learning_rate': 0.03,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'min_child_samples': 50,\n    'min_data_in_leaf': 100,\n    'subsample_for_bin': 200000,\n    'n_estimators': 15000,\n    'early_stopping_rounds': 50\n    ,'device_type': 'gpu'\n}\n\n# Create the LightGBM model object\nlgb_model = lgb.LGBMRegressor(**lgb_params)\n\n# Fit the model to the training data\nlgb_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=3000)\n\n# Generate predictions on the validation data\nval_pred_lgb = lgb_model.predict(X_val)\n\n# Calculate the RMSE\nval_rmse_lgb = rmse(y_val, val_pred_lgb)\nprint(\"lgb RMSE:\", val_rmse_lgb)\n\n# Make predictions on the competition test data\ntest_pred_lgb = lgb_model.predict(X_test_db[features])\n\n# models['lgb_model'] = (lgb_model, features)\n\nprint(\"Training and predicting using xgboost\")\nxgb_params = {'base_score': 0.5, \n          'booster': 'gbtree',\n          'tree_method': 'gpu_hist',\n          'n_estimators': 15000,\n          'objective': 'reg:squarederror',\n          'max_depth': 6,\n          'subsample': 0.5,\n          'colsample_bytree': 0.5,\n          'gamma': 1.4,\n          'min_child_weight': 7,\n          'learning_rate': 0.01,\n          'verbose':3000\n          ,'gpu_id': 0\n          }\n\nxgb_model = xgb.XGBRegressor(**xgb_params)\n\nxgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=3000)\n\n# make predictions on the validation data\nval_pred_xgb = xgb_model.predict(X_val)\n\n# calculate the RMSE\nval_rmse_xgb =rmse(y_val, val_pred_xgb)\nprint(\"xgb RMSE:\", val_rmse_xgb)\n\n\n# Make predictions on the competition test data\ntest_pred_xgb = xgb_model.predict(X_test_db[features])\n\n# models['xgb_model'] = (xgb_model, features)\n\ncb_params = {'iterations': 15000,\n          'learning_rate': 0.01,\n          'depth': 6,\n          'l2_leaf_reg': 3,\n          'bagging_temperature': 1,\n          'border_count': 256,\n          'loss_function': 'RMSE',\n          'random_seed': None,\n          'task_type': 'GPU',\n          'verbose': 3000}\n\n# Create the Catboost model object\ncb_model = CatBoostRegressor(**cb_params)\n\n# Fit the model to the training data\ncb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=3000)\n\n# Generate predictions on the validation data\nval_pred_cb = cb_model.predict(X_val)\n\n# Calculate the RMSE\nval_rmse_cb = rmse(y_val, val_pred_cb)\nprint(\"lgb RMSE:\", val_rmse_cb)\n\n# Make predictions on the competition test data\ntest_pred_cb = cb_model.predict(X_test_db[features])\n\n# models['cb_model'] = (cb_model, features)\n\nsubmission_db = X_test_db[['index']].copy()\nsubmission_db['xgb'] = test_pred_xgb\nsubmission_db['lgb'] = test_pred_lgb\nsubmission_db['cb'] = test_pred_cb\n\nsubmission_db.to_csv(fname+ 'submission_db_0.csv', index = False)\nprint(fname)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_db.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_db, y_db, X_test_db, X_train, X_val, y_train, y_val #, val_pred_xgb, test_pred_xgb, val_pred_lgb, test_pred_lgb,val_pred_cb, test_pred_cb\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def location_nom_wlf(train, test):\n    # Ref: https://www.kaggle.com/code/flaviafelicioni/wids-2023-different-locations-train-test-solved\n    scale = 14\n\n    train.loc[:,'lat']=round(train.lat,scale)\n    train.loc[:,'lon']=round(train.lon,scale)\n    test.loc[:,'lat']=round(test.lat,scale)\n    test.loc[:,'lon']=round(test.lon,scale)\n\n    all_df = pd.concat([train, test], axis=0)\n    all_df['loc_group'] = all_df.groupby(['lat','lon']).ngroup()\n    train = all_df.iloc[:len(train)]\n    test = all_df.iloc[len(train):].drop(target, axis=1)\n    \n    return train, test\n\ndef categorical_encode_wlf(train, test):\n    le = LabelEncoder()\n    train['climateregions__climateregion'] = le.fit_transform(train['climateregions__climateregion'])\n    test['climateregions__climateregion'] = le.transform(test['climateregions__climateregion'])\n    return train, test\n    \ndef fill_na_wlf(df):\n    df = df.sort_values(by=['loc_group', 'startdate']).ffill()\n    return df\n\ndef creat_new_featute_wlf(df):\n    df['year'] = df['startdate'].dt.year\n    df['month'] = df['startdate'].dt.month\n    df['day_of_year'] = df['startdate'].dt.dayofyear\n    return df\n\ndef feature_engineering_wlf(train_raw, test_raw):\n    train, test = location_nom_wlf(train_raw, test_raw)\n    train = fill_na_wlf(train)\n    train = creat_new_featute_wlf(train)\n    test = creat_new_featute_wlf(test)\n    train, test = categorical_encode_wlf(train, test)\n\n    drop_cols = ['index', 'startdate', 'lat', 'lon', target]\n    features = [col for col in train.columns if col not in drop_cols]\n    X = train[features]\n    X_test = test[features + ['index']]\n    y = train[target]\n\n    return X, y, X_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y, X_test = feature_engineering_wlf(train_raw.copy(), test_raw.copy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1, Y1 = X.copy(), y.copy()\nfeatures = X1.columns.tolist()\ncb_params = {\n  'iterations':2000,\n  'learning_rate' : 0.0980689972639084,\n#   'subsample' : 0.7443133148363695, \n  'l2_leaf_reg' : 2.3722386345448316,\n  'max_depth' : int(6.599144674342465),\n  'use_best_model' : True, \n  'loss_function' : 'RMSE',\n  'model_size_reg' : 0.4833187897595954,\n    'task_type': 'GPU',\n    'verbose':500\n }\n\ntrain_pool = Pool(data=X1,label = Y1)\nX_train, X_val, y_train, y_val = train_test_split(X1, Y1, test_size=0.33, random_state=42)\n\ncb_model = CatBoostRegressor(**cb_params)\ncb_model.fit(train_pool, eval_set=(X_val, y_val), plot=True)\n\n# make predictions on the validation data\nval_pred_cb = cb_model.predict(X_val)\n# calculate the RMSE\nval_rmse_cb =rmse(y_val, val_pred_cb)\nprint(\"cb RMSE:\", val_rmse_cb)\n\nprint(cb_model.get_best_score())\n\ntest_pred_cb  = cb_model.predict(X_test[features])\n# models['cb_model_0'] = (cb_model, features)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pseudo Labelling: Iteration 0\ntrain_pseudo = X_test[features].copy()\ntrain_pseudo[target] = test_pred_cb\nX_train_new = X1\nX_train_new[target] = Y1\n\ntrain_pseudo = pd.concat([X_train_new, train_pseudo], axis=0).reset_index(drop=True)\n\nX_PL0 = train_pseudo[features]\ny_PL0 = train_pseudo[target]\n\ndel train_pseudo, X_train_new\ngc.collect()\n\n# Pseudo Labeling with catboost model only\ntrain_pool = Pool(data=X_PL0, label = y_PL0)\n\nX_train, X_val, y_train, y_val = train_test_split(X_PL0, y_PL0, test_size=0.33, random_state=42)\n\n\ncb_model_PL0 = CatBoostRegressor(**cb_params)\ncb_model_PL0.fit(train_pool, eval_set=(X_val, y_val), plot=True)\n\n# make predictions on the validation data\nval_pred_cb_PL0 = cb_model_PL0.predict(X_val)\n\n# calculate the RMSE\nval_rmse_cb_PL0 =rmse(y_val, val_pred_cb_PL0)\nprint(\"cb RMSE:\", val_rmse_cb_PL0)\n\nprint(cb_model_PL0.get_best_score())\ntest_pred_cb_PL0  = cb_model_PL0.predict(X_test[features])\n\n# models['cb_model_PL0'] = (cb_model_PL0, features)\n\nsubmission_wlf = X_test[['index']].copy()\nsubmission_wlf['cb_PL0'] = test_pred_cb_PL0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble WLF catboost with Pseudo labeled model with DB xgboost and lightgbm models\nsubmission_wlf = submission_wlf.merge(submission_db, on = 'index')\n# weights= {'lgb':0.27,  'cb':0.48} # 'xgb':0.16,\n# weights= {'lgb':0.27, 'cb':0.40, 'cb_PL0':0.33} #  'xgb':0.16,\nweights= {'lgb':0.37, 'cb':0.31, 'cb_PL0':0.33} # 'xgb':0.16,\nsubmission_wlf['ensemble'] = (submission_wlf['lgb'] * weights['lgb'] + \n                            submission_wlf['cb'] * weights['cb'] + \n                            submission_wlf['cb_PL0'] * weights['cb_PL0'])\n\nsubmission_wlf.to_csv(fname+ 'submission_wlf_PL0_1.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del (X_PL0, y_PL0, train_pool, X_train, X_val, y_train, y_val, X1, Y1, \n     submission_db, val_pred_cb_PL0, test_pred_cb_PL0,\n     val_pred_xgb, test_pred_xgb, val_pred_lgb, test_pred_lgb,val_pred_cb, test_pred_cb)\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_wlf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #  Pseudo Labeling Iteration 1: Second Round Catboost model building and Pseudo Labeling with the target created above\n\nX1, Y1 = X.copy(), y.copy()\nfeatures = X1.columns.tolist()\ncb_params = {\n          'iterations':25000,\n          'verbose':3000,\n          'learning_rate' : 0.0980689972639084,\n          #'subsample' : 0.7443133148363695, \n          'l2_leaf_reg' : 2.3722386345448316,\n          'max_depth' : int(6.599144674342465),\n          'use_best_model' : True, \n          'loss_function' : 'RMSE',\n          'model_size_reg' : 0.4833187897595954,\n    'task_type': 'GPU',\n         }\n\n# Pseudo Labelling\npsedolb = submission_wlf[['index', 'ensemble']].copy()\npsedolb.rename(columns={'ensemble': target}, inplace=True)\n\ntrain_pseudo = X_test.copy()\ntrain_pseudo = train_pseudo.merge(psedolb, on = 'index')\ntrain_pseudo = train_pseudo[features + [target]]\nX_train_new= X1 \nX_train_new[target] = Y1\n\ntrain_pseudo = pd.concat([X_train_new, train_pseudo], axis=0).reset_index(drop=True)\n\nX_PL1 = train_pseudo[features]\ny_PL1 = train_pseudo[target]\n\ndel train_pseudo, X_train_new \ngc.collect()\n\n# Pseudo Labeling with catboost model only\ntrain_pool = Pool(data=X_PL1, label = y_PL1)\nX_train, X_val, y_train, y_val = train_test_split(X_PL1, y_PL1, test_size=0.33, random_state=42)\n\n\ncb_model_PL1 = CatBoostRegressor(**cb_params)\ncb_model_PL1.fit(train_pool, eval_set=(X_val, y_val), plot=True)\n\n# make predictions on the validation data\nval_pred_cb_PL1 = cb_model_PL1.predict(X_val)\n# calculate the RMSE\nval_rmse_cb_PL1 =rmse(y_val, val_pred_cb_PL1)\nprint(\"cb RMSE:\", val_rmse_cb_PL1)\n\nprint(cb_model_PL1.get_best_score())\ntest_pred_cb_PL1  = cb_model_PL1.predict(X_test[features])\n\n# models['cb_model_PL1'] = (cb_model_PL1, features)\n\nsubmission_wlf_pl1 = X_test[['index']].copy()\nsubmission_wlf_pl1['cb_PL1'] = test_pred_cb_PL1\n\nlgb_params = {'boosting_type': 'gbdt',\n          'objective': 'regression',\n          'metric': 'rmse',\n          'max_depth': 4,\n          'num_leaves': 31,\n          'learning_rate': 0.09,\n          'feature_fraction': 0.9,\n          'bagging_fraction': 0.8,\n          'bagging_freq': 5,\n          'early_stopping_round': 50\n              ,'device_type': 'gpu',\n          'n_estimators': 15000}\n\nlgb_model_PL1 = lgb.LGBMRegressor(**lgb_params)\nlgb_model_PL1.fit(X_PL1, y_PL1, eval_set=[(X_val, y_val)], verbose=3000)\n\n# make predictions on the validation data\nval_pred_lgb_PL1 = lgb_model_PL1.predict(X_val)\n# calculate the RMSE\nval_rmse_lgb_PL1 =rmse(y_val, val_pred_lgb_PL1)\nprint(\"lgb RMSE:\", val_rmse_lgb_PL1)\n\ntest_pred_lgb_PL1 = lgb_model_PL1.predict(X_test[features])\nsubmission_wlf_pl1['lgb_PL1'] = test_pred_lgb_PL1\n\nxgb_model_PL1 = xgb.XGBRegressor(base_score=0.5, \n                           n_estimators=20000,\n                           objective='reg:linear',\n                           max_depth=5,\n                           early_stopping_rounds=100,\n                           tree_method = 'gpu_hist',\n                           gpu_id= 0,\n                           learning_rate=0.01)\n\nxgb_model_PL1.fit(X_PL1, y_PL1, eval_set=[(X_val, y_val)], verbose=3000)\n\n# make predictions on the validation data\nval_pred_xgb_PL1 = xgb_model_PL1.predict(X_val)\n# calculate the RMSE\nval_rmse_xgb_PL1 =rmse(y_val, val_pred_xgb_PL1)\nprint(\"cb RMSE:\", val_rmse_xgb_PL1)\n\ntest_pred_xgb_PL1 = xgb_model_PL1.predict(X_test[features])\n\n# models['lgb_model_PL1'] = (lgb_model_PL1, features)\n\nsubmission_wlf_pl1['xgb_PL1'] = test_pred_xgb_PL1\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train, y_train, train_pool, X1, Y1 , X_PL1, y_PL1\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensemble weights\n# weights= {'lgb':0.43, 'xgb':0.07, 'cb':0.58}\nweights = {'lgb':0.37,  'cb':0.63} # 'xgb':0.07\nsubmission_wlf_pl1['ensemble'] = (submission_wlf_pl1['lgb_PL1'] * weights['lgb'] +  \n                                submission_wlf_pl1['cb_PL1'] * weights['cb']) # submission_wlf_pl1['xgb_PL1'] * weights['xgb']  +\nsubmission_wlf_pl1.to_csv(fname + 'submission_wlf_PL1_2.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_wlf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_wlf_pl1.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = submission_wlf[['index', 'ensemble']].merge(submission_wlf_pl1[['index', 'ensemble']], on = 'index', suffixes = ['_db_PL0', '_PL1'])\nweights = {'ensemble_db_PL0':0.15,  'ensemble_PL1':0.84, 'quality_threshold':0.5}\nsubmission['ensemble'] = np.where(abs(submission['ensemble_db_PL0'] - submission['ensemble_PL1']) < weights['quality_threshold'],\n                                  submission['ensemble_db_PL0'] * weights['ensemble_db_PL0'] + submission['ensemble_PL1'] * weights['ensemble_PL1'],\n                                 submission['ensemble_PL1'])\nsubmission[target] = submission['ensemble']\nsubmission[['index', target]].to_csv(fname+'submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pseudo Labelling Iteration 2:\n# Climate region optimization\nfrom tqdm.notebook import tqdm\n\nX1, Y1 = X.copy(), y.copy()\nfeatures = X1.columns.tolist()\n\npsedolb = submission[['index', target]].copy()\n\ntrain_pseudo = X_test.copy()\ntrain_pseudo = train_pseudo.merge(psedolb, on = 'index')\ntrain_pseudo = train_pseudo[features + [target]]\nX_train_new= X1 \nX_train_new[target] = Y1\n\ntrain_pseudo = pd.concat([X_train_new, train_pseudo], axis=0).reset_index(drop=True)\n\nX_PL1 = train_pseudo[features]\ny_PL1 = train_pseudo[target]\n\ndel train_pseudo, X_train_new \ngc.collect()\n\ncb_params = {'iterations': 10000,\n            'learning_rate': 0.05,\n            'depth': 4,\n            'l2_leaf_reg': 3,\n            'bagging_temperature': 0,\n            'border_count': 128,\n            'loss_function': 'RMSE',\n            'random_seed': 42,\n            'task_type': 'GPU',\n            'verbose': 5000}\n\nclimate = X_test[['index', 'climateregions__climateregion']].copy()\n\nfor REGION in X_PL1[\"climateregions__climateregion\"].unique().tolist():\n    print(REGION)\n    weights = X_PL1[\"climateregions__climateregion\"].apply(lambda x: 1 if x==REGION else 0.3)\n    train_pool = Pool(data=X_PL1, label = y_PL1, weight = weights)\n    cb_model = CatBoostRegressor(**cb_params) \n    cb_model.fit(train_pool, eval_set=(X_val, y_val))\n    f_name = target + str(REGION)\n    climate[f_name] = cb_model.predict(X_test[features])\n    \n\nclimate_regions = sorted(X[\"climateregions__climateregion\"].unique().tolist())\n\nclimate_map = {climate_regions[i]:i for i in range(len(climate_regions))}\n\nclimate_cols =  [target + str(climate_regions[i]) for i in range(len(climate_regions))] + ['index', 'climateregions__climateregion'] \n\nclimate['ensemble'] = climate[climate_cols].apply(lambda x: x[climate_map[x[-1]]], axis = 1)\nclimate['avg_climate'] = climate[climate_cols].apply(lambda x: np.mean(x), axis = 1)\n# climate.to_csv(fname + 'climate_3.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"climate_df = climate[['index', 'ensemble', 'avg_climate']].merge(submission_wlf_pl1, on = 'index', suffixes = ['_climate', '_db_PL0_PL1'])\n\n# ensemble weights\nweights= {'ensemble_climate':0.13, 'ensemble_db_PL0_PL1':0.87, 'quality_threshold': 0.6}\nweights= {'ensemble_climate':0.1, 'ensemble_db_PL0_PL1':0.9, 'quality_threshold': 0.6}\n\nclimate_df['ensemble'] = np.where(abs(climate_df['ensemble_db_PL0_PL1'] - climate_df['ensemble_climate']) < weights['quality_threshold'],  \n                              climate_df['ensemble_db_PL0_PL1']*weights['ensemble_db_PL0_PL1'] + climate_df['ensemble_climate'] * weights['ensemble_climate'],\n                              climate_df['ensemble_db_PL0_PL1'])\nclimate_df.to_csv(fname+ 'submission_wlf_pl1_ensembled_climate_4.csv',index=False)\nclimate_df[target] = climate_df['ensemble']\nclimate_df[['index', target]].to_csv('submission_climate.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"climate_df[['index', target]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}